{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5zPzKiQAW/0l9/XUcPVk5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nancytechie/Data-Science-/blob/main/AI_Lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgoY1osDQKR4",
        "outputId": "1c2b52f5-9bc4-4bd9-d8df-db1a105f19c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting advertools\n",
            "  Downloading advertools-0.13.5-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from advertools) (1.5.3)\n",
            "Requirement already satisfied: pyasn1>=0.4 in /usr/local/lib/python3.10/dist-packages (from advertools) (0.5.0)\n",
            "Collecting scrapy>=2.5.0 (from advertools)\n",
            "  Downloading Scrapy-2.11.0-py2.py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.4/286.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting twython>=3.8.0 (from advertools)\n",
            "  Downloading twython-3.9.1-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from advertools) (9.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->advertools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->advertools) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->advertools) (1.23.5)\n",
            "Collecting Twisted<23.8.0,>=18.9.0 (from scrapy>=2.5.0->advertools)\n",
            "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy>=2.5.0->advertools) (41.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy>=2.5.0->advertools)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy>=2.5.0->advertools)\n",
            "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy>=2.5.0->advertools)\n",
            "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy>=2.5.0->advertools) (23.2.0)\n",
            "Collecting queuelib>=1.4.2 (from scrapy>=2.5.0->advertools)\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy>=2.5.0->advertools)\n",
            "  Downloading service_identity-23.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy>=2.5.0->advertools)\n",
            "  Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy>=2.5.0->advertools)\n",
            "  Downloading zope.interface-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy>=2.5.0->advertools)\n",
            "  Downloading Protego-0.3.0-py2.py3-none-any.whl (8.5 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy>=2.5.0->advertools)\n",
            "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy>=2.5.0->advertools) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy>=2.5.0->advertools) (23.1)\n",
            "Collecting tldextract (from scrapy>=2.5.0->advertools)\n",
            "  Downloading tldextract-3.6.0-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy>=2.5.0->advertools) (4.9.3)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy>=2.5.0->advertools)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from twython>=3.8.0->advertools) (2.31.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from twython>=3.8.0->advertools) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy>=2.5.0->advertools) (1.15.1)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy>=2.5.0->advertools)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.0->advertools) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->twython>=3.8.0->advertools) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->twython>=3.8.0->advertools) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->twython>=3.8.0->advertools) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->twython>=3.8.0->advertools) (2023.7.22)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.4.0->twython>=3.8.0->advertools) (3.2.2)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy>=2.5.0->advertools) (23.1.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy>=2.5.0->advertools) (0.3.0)\n",
            "Collecting constantly>=15.1 (from Twisted<23.8.0,>=18.9.0->scrapy>=2.5.0->advertools)\n",
            "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting incremental>=21.3.0 (from Twisted<23.8.0,>=18.9.0->scrapy>=2.5.0->advertools)\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting Automat>=0.8.0 (from Twisted<23.8.0,>=18.9.0->scrapy>=2.5.0->advertools)\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted<23.8.0,>=18.9.0->scrapy>=2.5.0->advertools)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy>=2.5.0->advertools) (4.5.0)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy>=2.5.0->advertools)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy>=2.5.0->advertools) (3.12.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy>=2.5.0->advertools) (2.21)\n",
            "Installing collected packages: PyDispatcher, incremental, constantly, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, hyperlink, cssselect, Automat, Twisted, requests-file, parsel, twython, tldextract, service-identity, itemloaders, scrapy, advertools\n",
            "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 advertools-0.13.5 constantly-15.1.0 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 parsel-1.8.1 protego-0.3.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.11.0 service-identity-23.1.0 tldextract-3.6.0 twython-3.9.1 w3lib-2.1.2 zope.interface-6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install advertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ua-parser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6qw4mQtcla1",
        "outputId": "4062c671-193d-4fe7-fa86-6572cbf3c6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ua-parser\n",
            "  Downloading ua_parser-0.18.0-py2.py3-none-any.whl (38 kB)\n",
            "Installing collected packages: ua-parser\n",
            "Successfully installed ua-parser-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install squarify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiKrryfpWPJn",
        "outputId": "770377b1-6a70-40a2-f19f-564436e8258e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting squarify\n",
            "  Downloading squarify-0.4.3-py3-none-any.whl (4.3 kB)\n",
            "Installing collected packages: squarify\n",
            "Successfully installed squarify-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import advertools as adv\n",
        "import pandas as pd\n",
        "from ua_parser import user_agent_parser\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow\n",
        "from ipywidgets import interact\n",
        "import matplotlib.pyplot as plt\n",
        "import ua_parser\n",
        "pd.options.display.max_columns = None\n",
        "import squarify"
      ],
      "metadata": {
        "id": "8DuIjviAWUd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "adv.logs_to_df(\n",
        "    log_file='/content/log_file.csv',\n",
        "    output_file='output_file.parquet',\n",
        "    errors_file='errors_file.txt',\n",
        "    log_format='combined')"
      ],
      "metadata": {
        "id": "--Ny-YAjWYaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e22d59-fd78-464a-85b8-c84f4e4d6ab8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed           6,001 lines.\n",
            "CPU times: user 124 ms, sys: 30.9 ms, total: 155 ms\n",
            "Wall time: 352 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logs_df = pd.read_parquet('output_file.parquet')\n",
        "logs_df['datetime'] = pd.to_datetime(logs_df['datetime'],\n",
        "                                     format='%d/%b/%Y:%H:%M:%S %z')"
      ],
      "metadata": {
        "id": "_GO7jjPaYyhV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "# Create a dictionary to store log segments for each hour\n",
        "log_segments = defaultdict(list)\n",
        "\n",
        "# Regular expression to match the timestamp\n",
        "timestamp_pattern = r'\\[([^:]+):\\d{2}:\\d{2}:\\d{2}'\n",
        "\n",
        "for log_entry in logs_df:\n",
        "    # Extract the timestamp using regex\n",
        "    match = re.search(timestamp_pattern, log_entry)\n",
        "    if match:\n",
        "        timestamp = match.group(1)\n",
        "\n",
        "        # Extract the hour from the timestamp\n",
        "        hour = timestamp.split(':')[1]\n",
        "\n",
        "        # Append the log entry to the corresponding hour's segment\n",
        "        log_segments[hour].append(log_entry)\n",
        "\n",
        "# Print the log segments for each hour\n",
        "for hour, segment in log_segments.items():\n",
        "    print(f\"Hour: {hour}\")\n",
        "    for entry in segment:\n",
        "        print(entry)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "XZcm-eFaY-an"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import parse\n",
        "import os\n",
        "from flask import Flask, render_template, request, send_file"
      ],
      "metadata": {
        "id": "RVIWjYendUmV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/upload', methods=['POST'])\n",
        "def upload():\n",
        "    if 'log_file' not in request.files:\n",
        "        return \"No file part\"\n",
        "\n",
        "    log_file = request.files['log_file']\n",
        "\n",
        "    if log_file.filename == '':\n",
        "        return \"No selected file\"\n",
        "    if log_file:\n",
        "\n",
        "        file_contents = log_file.read()\n",
        "        with open('logfile.log', 'w') as log:\n",
        "            log.write(file_contents.decode('utf-8'))\n",
        "        adv.logs_to_df(\n",
        "            log_file='logfile.log',\n",
        "            output_file='output_file.parquet',\n",
        "            errors_file='errors_file.txt',\n",
        "            log_format='combined'\n",
        "        )\n",
        "        logs_df = pd.read_parquet('output_file.parquet')\n",
        "        logs_df['datetime'] = pd.to_datetime(logs_df['datetime'],\n",
        "                                    format='%d/%b/%Y:%H:%M:%S %z')\n",
        "        report = generate_report(logs_df)\n",
        "        report_filename = 'report.txt'\n",
        "        with open(report_filename, 'w') as report_file:\n",
        "            report_file.write(report)\n",
        "        report = generate_report(logs_df)\n",
        "    return send_file(report_filename, as_attachment=True, download_name='report.txt')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k6pkniYkdYsp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_report(logs_df):\n",
        "    # Example: Calculate the total number of log entries\n",
        "    total_entries = len(logs_df)\n",
        "\n",
        "    # Example: Calculate the unique IP addresses\n",
        "    unique_ips = logs_df['client'].nunique()\n",
        "\n",
        "    # Example: Calculate the number of HTTP status codes\n",
        "    status_code_counts = logs_df['status'].value_counts()\n",
        "    summary_stats=logs_df.describe()\n",
        "    info=logs_df.info()\n",
        "    inf=dataframe_to_text(info)\n",
        "    # Generate summary statistics for numeric columns\n",
        "    #numeric_summary = logs_df.describe(include='number')\n",
        "    ns=dataframe_to_text(summary_stats)\n",
        "    top_visitors = dataframe_to_text(logs_df['client'].value_counts().head(10))\n",
        "    frequency_of_requests = dataframe_to_text(logs_df['request'].value_counts().head(10))\n",
        "    status_df = dataframe_to_text(logs_df.groupby('status').size().reset_index(name='count'))\n",
        "    logs_df['hour'] = logs_df['datetime'].dt.hour\n",
        "    logs_df['day_of_week'] = logs_df['datetime'].dt.dayofweek\n",
        "    # Group by IP address and hour, and calculate the count of requests\n",
        "    ip_hourly_counts = dataframe_to_text(logs_df.groupby(['client', 'hour'])['hour'].count().reset_index(name='request_count').head(5))\n",
        "    # Group by IP address and day of the week, and calculate the average request count\n",
        "    ip_weekly_avg = dataframe_to_text(logs_df.groupby(['client', 'day_of_week'])['day_of_week'].count().reset_index(name='request_count').head(5))\n",
        "    logs_df['browser'] = logs_df['user_agent'].apply(extract_browser)\n",
        "    browser_hits = dataframe_to_text(logs_df.groupby('browser').size().reset_index(name='total_hits'))\n",
        "    browser_os_df = ua_df[['ua_family', 'ua_os.family']]\n",
        "    # Count the number of occurrences of each browser and operating system combination\n",
        "    browser_os_counts = browser_os_df.groupby(['ua_family', 'ua_os.family']).size().reset_index(name='count')\n",
        "    # Example: Generate a simple report\n",
        "    report = f\"Log Analysis Report\\n\\n\"\n",
        "    report += f\"Total Log Entries: {total_entries}\\n\"\n",
        "    report += f\"Unique IP Addresses: {unique_ips}\\n\"\n",
        "    report += \"HTTP Status Code Counts:\\n\"\n",
        "    for code, count in status_code_counts.items():\n",
        "        report += f\"  - {code}: {count}\\n\"\n",
        "    report += \"\\nSummary Statistics (Numeric Columns):\\n\"\n",
        "    report+=ns\n",
        "    report += \"\\nDataFrame Information:\\n\"\n",
        "    report+=inf\n",
        "    report+=\"\\nTop Visitors:\\n\"\n",
        "    report+=top_visitors\n",
        "    report+=\"\\nFrequency:\\n\"\n",
        "    report+=frequency_of_requests\n",
        "    report+=\"\\nStatus:\\n\"\n",
        "    report+=status_df\n",
        "    report+=\"\\nIP_Hourly_count:\\n\"\n",
        "    report+=ip_hourly_counts\n",
        "    report+=\"\\nIP Weekly Count:\\n\"\n",
        "    report+=ip_weekly_avg\n",
        "    report+=\"\\nBrowser Hits\\n\"\n",
        "    report+=browser_hits\n",
        "    return report"
      ],
      "metadata": {
        "id": "biuu7Ph8dfbl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def dataframe_to_text(df):\n",
        "    if df is None:\n",
        "        return \"No data available for report\"\n",
        "    return df.to_string(index=False)\n"
      ],
      "metadata": {
        "id": "mKGb4Mmkdlgr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_browser(user_agent):\n",
        "    ua = parse(user_agent)\n",
        "    browser_family = ua.browser.family.lower()\n",
        "\n",
        "    if 'chrome' in browser_family:\n",
        "        return 'Chrome'\n",
        "    elif 'firefox' in browser_family:\n",
        "        return 'Firefox'\n",
        "    elif 'safari' in browser_family:\n",
        "        return 'Safari'\n",
        "    elif 'edge' in browser_family:\n",
        "        return 'Edge'\n",
        "    elif 'opera' in browser_family:\n",
        "        return 'Opera'\n",
        "    elif 'brave' in browser_family:\n",
        "        return 'Brave'\n",
        "    elif 'internet explorer' in browser_family or 'msie' in browser_family:\n",
        "        return 'Internet Explorer'\n",
        "    else:\n",
        "        return 'Other'\n",
        "if __name__ == '_main_':\n",
        "    app.run(debug=True)\n",
        ""
      ],
      "metadata": {
        "id": "2FsME40Idn8n"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}